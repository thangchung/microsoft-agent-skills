# Test scenarios for azure-ai-projects-py skill evaluation
# Each scenario tests a specific usage pattern against acceptance criteria

config:
  model: gpt-4
  max_tokens: 2000
  temperature: 0.3

scenarios:
  # Client Creation and Setup
  - name: client_creation
    prompt: |
      Create an AIProjectClient with proper authentication using DefaultAzureCredential.
      Use environment variables for the endpoint and include a context manager pattern.
    expected_patterns:
      - "from azure.ai.projects import AIProjectClient"
      - "from azure.identity import DefaultAzureCredential"
      - "DefaultAzureCredential()"
      - "endpoint="
      - "credential="
      - "with project_client:"
    forbidden_patterns:
      - "from azure.ai.projects.models import AIProjectClient"  # Wrong location
      - "from azure.ai.projects import AgentsClient"  # Wrong class
      - "url="  # Wrong parameter name
      - "project_endpoint="  # Wrong parameter name
    tags:
      - basic
      - authentication
      - client
    mock_response: |
      import os
      from azure.ai.projects import AIProjectClient
      from azure.identity import DefaultAzureCredential
      
      project_client = AIProjectClient(
          endpoint=os.environ["AZURE_AI_PROJECT_ENDPOINT"],
          credential=DefaultAzureCredential(),
      )
      
      with project_client:
          # Use project_client for all operations
          connections = project_client.connections.list()
          for conn in connections:
              print(f"Connection: {conn.name}")

  # Versioned Agent with PromptAgentDefinition
  - name: versioned_agent_creation
    prompt: |
      Create a versioned agent using create_version() with PromptAgentDefinition.
      Include a version label and description. The agent should be named "customer-support-agent".
    expected_patterns:
      - "from azure.ai.projects.models import PromptAgentDefinition"
      - "create_version"
      - "agent_name="
      - "definition=PromptAgentDefinition"
      - "version_label="
      - "model="
      - "instructions="
    forbidden_patterns:
      - "from azure.ai.agents.models import PromptAgentDefinition"  # Wrong module
    tags:
      - agents
      - versioned
    mock_response: |
      import os
      from azure.ai.projects import AIProjectClient
      from azure.ai.projects.models import PromptAgentDefinition
      from azure.identity import DefaultAzureCredential
      
      project_client = AIProjectClient(
          endpoint=os.environ["AZURE_AI_PROJECT_ENDPOINT"],
          credential=DefaultAzureCredential(),
      )
      
      with project_client:
          agent = project_client.agents.create_version(
              agent_name="customer-support-agent",
              definition=PromptAgentDefinition(
                  model=os.environ["AZURE_AI_MODEL_DEPLOYMENT_NAME"],
                  instructions="You are a customer support specialist.",
                  tools=[],
              ),
              version_label="v1.0",
              description="Initial version",
          )
          print(f"Agent: id={agent.id}, name={agent.name}, version={agent.version}")
          
          # Clean up
          project_client.agents.delete_agent(agent.id)

  # Agent with Code Interpreter Tool
  - name: agent_with_code_interpreter
    prompt: |
      Create an agent with CodeInterpreterTool that can execute Python code.
      Upload a data file and attach it to the code interpreter.
    expected_patterns:
      - "from azure.ai.agents.models import CodeInterpreterTool"
      - "CodeInterpreterTool()"
      - "FilePurpose.AGENTS"
      - "upload_and_poll"
      - "tool_resources"
      - "code_interpreter"
      - "file_ids"
    forbidden_patterns:
      - "from azure.ai.projects.models import CodeInterpreterTool"  # Wrong module!
      - "from azure.ai.projects import CodeInterpreterTool"  # Wrong module!
      - "FilePurpose.ASSISTANTS"  # Doesn't exist
      - 'tools=["code_interpreter"]'  # Should use CodeInterpreterTool class
    tags:
      - tools
      - code-interpreter
    mock_response: |
      import os
      from azure.ai.projects import AIProjectClient
      from azure.ai.agents.models import CodeInterpreterTool, FilePurpose
      from azure.identity import DefaultAzureCredential
      
      project_client = AIProjectClient(
          endpoint=os.environ["AZURE_AI_PROJECT_ENDPOINT"],
          credential=DefaultAzureCredential(),
      )
      
      with project_client:
          # Upload file for code interpreter
          file = project_client.agents.files.upload_and_poll(
              file_path="data.csv",
              purpose=FilePurpose.AGENTS,
          )
          
          code_interpreter = CodeInterpreterTool()
          
          agent = project_client.agents.create_agent(
              model=os.environ["AZURE_AI_MODEL_DEPLOYMENT_NAME"],
              name="data-analyst",
              instructions="Analyze data and create visualizations.",
              tools=code_interpreter.definitions,
              tool_resources={"code_interpreter": {"file_ids": [file.id]}},
          )
          
          print(f"Created agent with code interpreter: {agent.id}")
          
          # Clean up
          project_client.agents.delete_agent(agent.id)

  # Agent with File Search and Vector Store
  - name: agent_with_file_search
    prompt: |
      Create an agent with FileSearchTool for RAG over documents.
      Upload a document, create a vector store, and configure the agent to use it.
    expected_patterns:
      - "from azure.ai.agents.models import FileSearchTool"
      - "FileSearchTool"
      - "vector_stores.create_and_poll"
      - "vector_store_ids"
      - "file_search"
      - "tool_resources"
    forbidden_patterns:
      - "from azure.ai.projects.models import FileSearchTool"  # Wrong module!
      - "from azure.ai.projects import FileSearchTool"  # Wrong module!
    tags:
      - tools
      - file-search
      - rag
    mock_response: |
      import os
      from azure.ai.projects import AIProjectClient
      from azure.ai.agents.models import FileSearchTool, FilePurpose
      from azure.identity import DefaultAzureCredential
      
      project_client = AIProjectClient(
          endpoint=os.environ["AZURE_AI_PROJECT_ENDPOINT"],
          credential=DefaultAzureCredential(),
      )
      
      with project_client:
          # Upload document
          file = project_client.agents.files.upload_and_poll(
              file_path="./data/product_info.md",
              purpose=FilePurpose.AGENTS,
          )
          
          # Create vector store
          vector_store = project_client.agents.vector_stores.create_and_poll(
              file_ids=[file.id],
              name="product-docs",
          )
          
          # Create file search tool
          file_search = FileSearchTool(vector_store_ids=[vector_store.id])
          
          agent = project_client.agents.create_agent(
              model=os.environ["AZURE_AI_MODEL_DEPLOYMENT_NAME"],
              name="search-agent",
              instructions="Search uploaded files to answer questions.",
              tools=file_search.definitions,
              tool_resources=file_search.resources,
          )
          
          print(f"Created RAG agent: {agent.id}")
          
          # Clean up
          project_client.agents.delete_agent(agent.id)

  # Full Thread/Message/Run Conversation Flow
  - name: thread_message_run
    prompt: |
      Create a complete conversation flow: create a thread, add a user message,
      run the agent, and retrieve the assistant's response. Check the run status.
    expected_patterns:
      - "threads.create()"
      - "messages.create"
      - "thread_id="
      - 'role="user"'
      - "runs.create_and_process"
      - "agent_id="
      - "run.status"
      - "messages.list"
    forbidden_patterns:
      - 'role="human"'  # Must be "user"
    tags:
      - conversation
      - threads
      - messages
      - runs
    mock_response: |
      import os
      from azure.ai.projects import AIProjectClient
      from azure.identity import DefaultAzureCredential
      
      project_client = AIProjectClient(
          endpoint=os.environ["AZURE_AI_PROJECT_ENDPOINT"],
          credential=DefaultAzureCredential(),
      )
      
      with project_client:
          # Create agent
          agent = project_client.agents.create_agent(
              model=os.environ["AZURE_AI_MODEL_DEPLOYMENT_NAME"],
              name="conversation-agent",
              instructions="You are a helpful assistant.",
          )
          
          # Create thread
          thread = project_client.agents.threads.create()
          print(f"Created thread: {thread.id}")
          
          # Add user message
          message = project_client.agents.messages.create(
              thread_id=thread.id,
              role="user",
              content="What is the capital of Japan?",
          )
          
          # Run agent
          run = project_client.agents.runs.create_and_process(
              thread_id=thread.id,
              agent_id=agent.id,
          )
          print(f"Run finished with status: {run.status}")
          
          # Get response
          if run.status == "completed":
              messages = project_client.agents.messages.list(thread_id=thread.id)
              for msg in messages:
                  if msg.role == "assistant":
                      for content in msg.content:
                          if hasattr(content, 'text'):
                              print(f"Assistant: {content.text.value}")
          elif run.status == "failed":
              print(f"Run failed: {run.last_error}")
          
          # Clean up
          project_client.agents.delete_agent(agent.id)

  # Streaming with AgentEventHandler
  - name: streaming_with_handler
    prompt: |
      Create an agent that uses streaming responses with AgentEventHandler.
      Implement on_message_delta to print text as it arrives.
    expected_patterns:
      - "from azure.ai.agents.models import AgentEventHandler"
      - "class"
      - "AgentEventHandler"
      - "on_message_delta"
      - "runs.stream"
      - "event_handler="
      - "until_done"
    forbidden_patterns:
      - "from azure.ai.projects.models import AgentEventHandler"  # Wrong module
      - "stream=True"  # Old pattern
    tags:
      - streaming
      - advanced
    mock_response: |
      import os
      from azure.ai.projects import AIProjectClient
      from azure.ai.agents.models import AgentEventHandler
      from azure.identity import DefaultAzureCredential
      
      class MyHandler(AgentEventHandler):
          def on_message_delta(self, delta):
              if delta.text:
                  print(delta.text.value, end="", flush=True)
          
          def on_error(self, data):
              print(f"Error: {data}")
      
      project_client = AIProjectClient(
          endpoint=os.environ["AZURE_AI_PROJECT_ENDPOINT"],
          credential=DefaultAzureCredential(),
      )
      
      with project_client:
          agent = project_client.agents.create_agent(
              model=os.environ["AZURE_AI_MODEL_DEPLOYMENT_NAME"],
              name="streaming-agent",
              instructions="You are a helpful assistant.",
          )
          
          thread = project_client.agents.threads.create()
          
          project_client.agents.messages.create(
              thread_id=thread.id,
              role="user",
              content="Tell me a story.",
          )
          
          with project_client.agents.runs.stream(
              thread_id=thread.id,
              agent_id=agent.id,
              event_handler=MyHandler(),
          ) as stream:
              stream.until_done()
          
          project_client.agents.delete_agent(agent.id)

  # Connections Listing and Filtering
  - name: connections_listing
    prompt: |
      List project connections and filter by connection type.
      Show how to get a specific connection by name and filter by ConnectionType.AZURE_OPEN_AI.
    expected_patterns:
      - "from azure.ai.projects.models import ConnectionType"
      - "connections.list"
      - "ConnectionType.AZURE_OPEN_AI"
      - "connections.get"
      - "connection_name="
    forbidden_patterns:
      - 'connection_type="AzureOpenAI"'  # Must use enum, not string
      - "connections.get(name="  # Wrong parameter, should be connection_name=
    tags:
      - connections
    mock_response: |
      import os
      from azure.ai.projects import AIProjectClient
      from azure.ai.projects.models import ConnectionType
      from azure.identity import DefaultAzureCredential
      
      project_client = AIProjectClient(
          endpoint=os.environ["AZURE_AI_PROJECT_ENDPOINT"],
          credential=DefaultAzureCredential(),
      )
      
      with project_client:
          # List all connections
          print("All connections:")
          for conn in project_client.connections.list():
              print(f"  {conn.name}: {conn.connection_type}")
          
          # Filter by type
          print("\nAzure OpenAI connections:")
          for conn in project_client.connections.list(
              connection_type=ConnectionType.AZURE_OPEN_AI
          ):
              print(f"  {conn.name}")
          
          # Get specific connection by name
          connection = project_client.connections.get(
              connection_name="my-aoai-connection"
          )
          print(f"\nConnection details: {connection.name}, type: {connection.connection_type}")

  # Deployments Listing
  - name: deployments_listing
    prompt: |
      List model deployments in the project. Filter by model publisher and model name.
      Show how to dynamically select a GPT-4 deployment for agent creation.
    expected_patterns:
      - "deployments.list"
      - "model_publisher"
      - "model_name"
      - "deployment.name"
    forbidden_patterns:
      - 'deployment\.model\)'  # Wrong! Use model_name (but not model_name or model_publisher)
    tags:
      - deployments
    mock_response: |
      import os
      from azure.ai.projects import AIProjectClient
      from azure.identity import DefaultAzureCredential
      
      project_client = AIProjectClient(
          endpoint=os.environ["AZURE_AI_PROJECT_ENDPOINT"],
          credential=DefaultAzureCredential(),
      )
      
      with project_client:
          # List all deployments
          print("All deployments:")
          for deployment in project_client.deployments.list():
              print(f"  {deployment.name}: {deployment.model_name} ({deployment.model_publisher})")
          
          # Filter by publisher
          print("\nOpenAI deployments:")
          for deployment in project_client.deployments.list(model_publisher="OpenAI"):
              print(f"  {deployment.name}: {deployment.model_name}")
          
          # Filter by model name
          print("\nGPT-4o deployments:")
          for deployment in project_client.deployments.list(model_name="gpt-4o"):
              print(f"  {deployment.name}")
          
          # Dynamic model selection for agent
          gpt4_deployments = [
              d for d in project_client.deployments.list()
              if "gpt-4" in d.model_name.lower()
          ]
          
          if gpt4_deployments:
              deployment_name = gpt4_deployments[0].name
              
              agent = project_client.agents.create_agent(
                  model=deployment_name,
                  name="dynamic-agent",
                  instructions="You are helpful.",
              )
              print(f"\nCreated agent using deployment: {deployment_name}")
              project_client.agents.delete_agent(agent.id)

  # OpenAI Client and Evaluations
  - name: openai_client_evals
    prompt: |
      Get an OpenAI client from the project client and create an evaluation.
      Define a DataSourceConfigCustom with an item schema and testing criteria
      using built-in evaluators like builtin.fluency.
    expected_patterns:
      - "get_openai_client"
      - "from azure.ai.projects.models import DataSourceConfigCustom"
      - "DataSourceConfigCustom"
      - 'type="custom"'
      - "item_schema"
      - "openai_client.evals.create"
      - "testing_criteria"
      - "builtin."
    forbidden_patterns:
      - "project_client.evals.create"  # Wrong! evals are on openai_client
      - 'type="json"'  # Wrong type value
    tags:
      - evaluations
      - openai
    mock_response: |
      import os
      from azure.ai.projects import AIProjectClient
      from azure.ai.projects.models import DataSourceConfigCustom
      from azure.identity import DefaultAzureCredential
      
      project_client = AIProjectClient(
          endpoint=os.environ["AZURE_AI_PROJECT_ENDPOINT"],
          credential=DefaultAzureCredential(),
      )
      
      with project_client:
          # Get OpenAI client
          openai_client = project_client.get_openai_client()
          
          # Define data source configuration
          data_source_config = DataSourceConfigCustom(
              type="custom",
              item_schema={
                  "type": "object",
                  "properties": {
                      "query": {"type": "string"},
                      "expected_response": {"type": "string"},
                  },
                  "required": ["query"],
              },
              include_sample_schema=True,
          )
          
          # Define testing criteria with built-in evaluators
          testing_criteria = [
              {
                  "type": "azure_ai_evaluator",
                  "name": "fluency_check",
                  "evaluator_name": "builtin.fluency",
                  "data_mapping": {
                      "query": "{{item.query}}",
                      "response": "{{item.response}}",
                  },
              },
              {
                  "type": "azure_ai_evaluator",
                  "name": "task_adherence",
                  "evaluator_name": "builtin.task_adherence",
                  "data_mapping": {
                      "query": "{{item.query}}",
                      "response": "{{item.response}}",
                  },
              },
          ]
          
          # Create evaluation
          eval_object = openai_client.evals.create(
              name="Agent Quality Evaluation",
              data_source_config=data_source_config,
              testing_criteria=testing_criteria,
          )
          print(f"Created evaluation: {eval_object.id}")

  # Bing Grounding Tool (Project-Level)
  - name: bing_grounding_tool
    prompt: |
      Create a versioned agent with BingGroundingAgentTool for web search capabilities.
      Use the project-level tool with BingGroundingSearchConfiguration.
    expected_patterns:
      - "from azure.ai.projects.models import"
      - "BingGroundingAgentTool"
      - "BingGroundingSearchToolParameters"
      - "BingGroundingSearchConfiguration"
      - "project_connection_id"
      - "search_configurations"
      - "PromptAgentDefinition"
    forbidden_patterns:
      - "from azure.ai.agents.models import BingGroundingAgentTool"  # This is the low-level version
    tags:
      - tools
      - bing
      - web-search
    mock_response: |
      import os
      from azure.ai.projects import AIProjectClient
      from azure.ai.projects.models import (
          PromptAgentDefinition,
          BingGroundingAgentTool,
          BingGroundingSearchToolParameters,
          BingGroundingSearchConfiguration,
      )
      from azure.identity import DefaultAzureCredential
      
      project_client = AIProjectClient(
          endpoint=os.environ["AZURE_AI_PROJECT_ENDPOINT"],
          credential=DefaultAzureCredential(),
      )
      
      with project_client:
          # Get Bing connection
          bing_connection = project_client.connections.get(
              connection_name=os.environ["BING_CONNECTION_NAME"]
          )
          
          # Create agent with Bing grounding
          agent = project_client.agents.create_version(
              agent_name="bing-search-agent",
              definition=PromptAgentDefinition(
                  model=os.environ["AZURE_AI_MODEL_DEPLOYMENT_NAME"],
                  instructions="You are a helpful assistant with web search capabilities.",
                  tools=[
                      BingGroundingAgentTool(
                          bing_grounding=BingGroundingSearchToolParameters(
                              search_configurations=[
                                  BingGroundingSearchConfiguration(
                                      project_connection_id=bing_connection.id
                                  )
                              ]
                          )
                      )
                  ],
              ),
          )
          print(f"Created Bing-enabled agent: {agent.id}")
          
          # Clean up
          project_client.agents.delete_agent(agent.id)

  # Azure AI Search Tool
  - name: azure_ai_search_tool
    prompt: |
      Create a versioned agent with AzureAISearchAgentTool for enterprise search.
      Configure the tool with an index resource and query type.
    expected_patterns:
      - "from azure.ai.projects.models import"
      - "AzureAISearchAgentTool"
      - "AzureAISearchToolResource"
      - "AISearchIndexResource"
      - "AzureAISearchQueryType"
      - "project_connection_id"
      - "index_name"
      - "query_type"
    forbidden_patterns:
      - "from azure.ai.agents.models import AzureAISearchAgentTool"  # Wrong module
    tags:
      - tools
      - azure-search
      - enterprise
    mock_response: |
      import os
      from azure.ai.projects import AIProjectClient
      from azure.ai.projects.models import (
          AzureAISearchAgentTool,
          AzureAISearchToolResource,
          AISearchIndexResource,
          AzureAISearchQueryType,
          PromptAgentDefinition,
      )
      from azure.identity import DefaultAzureCredential
      
      project_client = AIProjectClient(
          endpoint=os.environ["AZURE_AI_PROJECT_ENDPOINT"],
          credential=DefaultAzureCredential(),
      )
      
      with project_client:
          # Get search connection
          search_connection = project_client.connections.get(
              connection_name=os.environ["AI_SEARCH_CONNECTION_NAME"]
          )
          
          # Create agent with Azure AI Search
          agent = project_client.agents.create_version(
              agent_name="enterprise-search-agent",
              definition=PromptAgentDefinition(
                  model=os.environ["AZURE_AI_MODEL_DEPLOYMENT_NAME"],
                  instructions="""You are a helpful assistant. Always provide citations 
                  using format: [message_idx:search_idx source].""",
                  tools=[
                      AzureAISearchAgentTool(
                          azure_ai_search=AzureAISearchToolResource(
                              indexes=[
                                  AISearchIndexResource(
                                      project_connection_id=search_connection.id,
                                      index_name=os.environ["AI_SEARCH_INDEX_NAME"],
                                      query_type=AzureAISearchQueryType.VECTOR_SEMANTIC_HYBRID,
                                  ),
                              ]
                          )
                      )
                  ],
              ),
          )
          print(f"Created search agent: {agent.id}")
          
          # Clean up
          project_client.agents.delete_agent(agent.id)

  # Async Operations
  - name: async_operations
    prompt: |
      Create an async version of an Azure AI Projects client using the async client
      with proper context management, await patterns, and AsyncAgentEventHandler for streaming.
    expected_patterns:
      - "from azure.ai.projects.aio import AIProjectClient"
      - "from azure.identity.aio import DefaultAzureCredential"
      - "async with"
      - "await client.agents.create_agent"
      - "await client.agents.threads.create"
      - "AsyncAgentEventHandler"
      - "async def"
      - "asyncio.run"
    forbidden_patterns:
      - "from azure.ai.projects import AIProjectClient"  # Should use .aio for async
      - "from azure.identity import DefaultAzureCredential"  # Should use .aio for async
      - "from azure.ai.agents.models import AgentEventHandler"  # Sync handler with async!
    tags:
      - async
      - streaming
      - advanced
    mock_response: |
      import os
      import asyncio
      from azure.ai.projects.aio import AIProjectClient
      from azure.ai.agents.aio import AsyncAgentEventHandler
      from azure.identity.aio import DefaultAzureCredential
      
      class AsyncHandler(AsyncAgentEventHandler):
          async def on_message_delta(self, delta):
              if delta.text:
                  print(delta.text.value, end="", flush=True)
          
          async def on_error(self, data):
              print(f"Error: {data}")
      
      async def main():
          async with (
              DefaultAzureCredential() as credential,
              AIProjectClient(
                  endpoint=os.environ["AZURE_AI_PROJECT_ENDPOINT"],
                  credential=credential,
              ) as client,
          ):
              # Create agent
              agent = await client.agents.create_agent(
                  model=os.environ["AZURE_AI_MODEL_DEPLOYMENT_NAME"],
                  name="async-agent",
                  instructions="You are a helpful assistant.",
              )
              
              # Create thread
              thread = await client.agents.threads.create()
              
              # Add message
              await client.agents.messages.create(
                  thread_id=thread.id,
                  role="user",
                  content="Tell me a short story.",
              )
              
              # Stream response
              async with client.agents.runs.stream(
                  thread_id=thread.id,
                  agent_id=agent.id,
                  event_handler=AsyncHandler(),
              ) as stream:
                  await stream.until_done()
              
              # Clean up
              await client.agents.delete_agent(agent.id)
      
      asyncio.run(main())

  # Evaluation with Inline Data and Run
  - name: evaluation_with_inline_data
    prompt: |
      Create a complete evaluation workflow: define inline JSONL data,
      create an evaluation with testing criteria, run the evaluation,
      poll for completion, and retrieve results.
    expected_patterns:
      - "CreateEvalJSONLRunDataSourceParam"
      - "SourceFileContent"
      - "SourceFileContentContent"
      - "openai_client.evals.create"
      - "openai_client.evals.runs.create"
      - "evals.runs.retrieve"
      - "evals.runs.output_items.list"
      - "eval_id="
      - "run_id="
      - "run.status"
    forbidden_patterns:
      - "from azure.ai.evaluation import evaluate"  # Deprecated SDK
      - "from azure.ai.evaluation import"  # Deprecated SDK
    tags:
      - evaluations
      - runs
    mock_response: |
      import os
      import time
      from azure.ai.projects import AIProjectClient
      from azure.identity import DefaultAzureCredential
      from openai.types.evals.create_eval_jsonl_run_data_source_param import (
          CreateEvalJSONLRunDataSourceParam,
          SourceFileContent,
          SourceFileContentContent,
      )
      from openai.types.eval_create_params import DataSourceConfigCustom
      
      with (
          DefaultAzureCredential() as credential,
          AIProjectClient(
              endpoint=os.environ["AZURE_AI_PROJECT_ENDPOINT"],
              credential=credential,
          ) as project_client,
      ):
          openai_client = project_client.get_openai_client()
          
          # Prepare inline test data
          data = [
              {"query": "What is Azure?", "response": "Azure is Microsoft's cloud."},
              {"query": "What is AI?", "response": "AI is artificial intelligence."},
          ]
          
          data_source = CreateEvalJSONLRunDataSourceParam(
              type="jsonl",
              source=SourceFileContent(
                  type="file_content",
                  content=[
                      SourceFileContentContent(item=item, sample={})
                      for item in data
                  ],
              ),
          )
          
          data_source_config = DataSourceConfigCustom(
              type="custom",
              item_schema={
                  "type": "object",
                  "properties": {
                      "query": {"type": "string"},
                      "response": {"type": "string"},
                  },
                  "required": ["query", "response"],
              },
              include_sample_schema=False,
          )
          
          testing_criteria = [
              {
                  "type": "azure_ai_evaluator",
                  "name": "coherence",
                  "evaluator_name": "builtin.coherence",
                  "data_mapping": {
                      "query": "{{item.query}}",
                      "response": "{{item.response}}",
                  },
                  "initialization_parameters": {"deployment_name": "gpt-4o-mini"},
              },
          ]
          
          # Create evaluation
          eval_object = openai_client.evals.create(
              name="Quality Evaluation",
              data_source_config=data_source_config,
              testing_criteria=testing_criteria,
          )
          
          # Run evaluation
          run = openai_client.evals.runs.create(
              eval_id=eval_object.id,
              name="Run 1",
              data_source=data_source,
          )
          
          # Poll for completion
          while run.status not in ["completed", "failed", "cancelled"]:
              time.sleep(5)
              run = openai_client.evals.runs.retrieve(
                  eval_id=eval_object.id,
                  run_id=run.id,
              )
          
          # Retrieve results
          output_items = list(openai_client.evals.runs.output_items.list(
              eval_id=eval_object.id,
              run_id=run.id,
          ))
          
          for item in output_items:
              for result in item.results:
                  print(f"{result.name}: {result.score}")

  # Agent Evaluation with Sample Mapping
  - name: agent_evaluation_with_sample
    prompt: |
      Create an evaluation for an AI agent that uses sample mapping for agent outputs.
      Include tool call data in the sample and use agent evaluators like intent_resolution
      and tool_call_accuracy.
    expected_patterns:
      - "SourceFileContentContent"
      - "sample="
      - "output_text"
      - "output_items"
      - "include_sample_schema=True"
      - "builtin.intent_resolution"
      - "{{sample.output_text}}"
    forbidden_patterns:
      - "include_sample_schema=False"  # Must be True for agent evals
      - "from azure.ai.evaluation import"  # Deprecated SDK
    tags:
      - evaluations
      - agents
    mock_response: |
      import os
      from azure.ai.projects import AIProjectClient
      from azure.identity import DefaultAzureCredential
      from openai.types.evals.create_eval_jsonl_run_data_source_param import (
          CreateEvalJSONLRunDataSourceParam,
          SourceFileContent,
          SourceFileContentContent,
      )
      from openai.types.eval_create_params import DataSourceConfigCustom
      
      with (
          DefaultAzureCredential() as credential,
          AIProjectClient(
              endpoint=os.environ["AZURE_AI_PROJECT_ENDPOINT"],
              credential=credential,
          ) as project_client,
      ):
          openai_client = project_client.get_openai_client()
          
          # Agent evaluation data with tool calls
          data_source = CreateEvalJSONLRunDataSourceParam(
              type="jsonl",
              source=SourceFileContent(
                  type="file_content",
                  content=[
                      SourceFileContentContent(
                          item={"query": "What's the weather in Seattle?"},
                          sample={
                              "output_text": "It's 55Â°F and cloudy in Seattle.",
                              "output_items": [
                                  {
                                      "type": "tool_call",
                                      "name": "get_weather",
                                      "arguments": {"location": "Seattle"},
                                      "result": {"temp": "55", "condition": "cloudy"},
                                  }
                              ],
                          },
                      ),
                  ],
              ),
          )
          
          data_source_config = DataSourceConfigCustom(
              type="custom",
              item_schema={
                  "type": "object",
                  "properties": {"query": {"type": "string"}},
                  "required": ["query"],
              },
              include_sample_schema=True,  # Required for agent evaluations
          )
          
          testing_criteria = [
              {
                  "type": "azure_ai_evaluator",
                  "name": "intent_resolution",
                  "evaluator_name": "builtin.intent_resolution",
                  "data_mapping": {
                      "query": "{{item.query}}",
                      "response": "{{sample.output_text}}",
                  },
                  "initialization_parameters": {"deployment_name": "gpt-4o-mini"},
              },
              {
                  "type": "azure_ai_evaluator",
                  "name": "tool_call_accuracy",
                  "evaluator_name": "builtin.tool_call_accuracy",
                  "data_mapping": {
                      "query": "{{item.query}}",
                      "response": "{{sample.output_items}}",
                  },
                  "initialization_parameters": {"deployment_name": "gpt-4o-mini"},
              },
          ]
          
          eval_object = openai_client.evals.create(
              name="Agent Evaluation",
              data_source_config=data_source_config,
              testing_criteria=testing_criteria,
          )
          print(f"Created agent evaluation: {eval_object.id}")

  # Custom Code-Based Evaluator
  - name: custom_code_evaluator
    prompt: |
      Create a custom code-based evaluator using CodeBasedEvaluatorDefinition.
      The evaluator should count words in the response and check if it's concise.
    expected_patterns:
      - "from azure.ai.projects.models import"
      - "EvaluatorVersion"
      - "EvaluatorType.CUSTOM"
      - "CodeBasedEvaluatorDefinition"
      - "EvaluatorMetric"
      - "code_text="
      - "def grade"
      - "project_client.evaluators.create_version"
      - "data_schema"
      - "metrics"
    forbidden_patterns:
      - "from azure.ai.evaluation import evaluator"  # Deprecated decorator
      - "@evaluator"  # Deprecated decorator
    tags:
      - evaluations
      - custom
    mock_response: |
      import os
      from azure.ai.projects import AIProjectClient
      from azure.ai.projects.models import (
          EvaluatorVersion,
          EvaluatorCategory,
          EvaluatorType,
          CodeBasedEvaluatorDefinition,
          EvaluatorMetric,
          EvaluatorMetricType,
          EvaluatorMetricDirection,
      )
      from azure.identity import DefaultAzureCredential
      
      with (
          DefaultAzureCredential() as credential,
          AIProjectClient(
              endpoint=os.environ["AZURE_AI_PROJECT_ENDPOINT"],
              credential=credential,
          ) as project_client,
      ):
          evaluator = project_client.evaluators.create_version(
              name="word_count_evaluator",
              evaluator_version=EvaluatorVersion(
                  evaluator_type=EvaluatorType.CUSTOM,
                  categories=[EvaluatorCategory.QUALITY],
                  display_name="Word Count",
                  description="Counts words in response",
                  definition=CodeBasedEvaluatorDefinition(
                      code_text='def grade(sample, item) -> dict:\n    response = item.get("response", "")\n    word_count = len(response.split())\n    return {"word_count": word_count, "is_concise": word_count < 100}',
                      data_schema={
                          "type": "object",
                          "properties": {"response": {"type": "string"}},
                          "required": ["response"],
                      },
                      metrics={
                          "word_count": EvaluatorMetric(
                              type=EvaluatorMetricType.ORDINAL,
                              desirable_direction=EvaluatorMetricDirection.DECREASE,
                              min_value=0,
                              max_value=10000,
                          ),
                          "is_concise": EvaluatorMetric(
                              type=EvaluatorMetricType.BINARY,
                          ),
                      },
                  ),
              ),
          )
          print(f"Created evaluator: {evaluator.name} (version {evaluator.version})")

  # Safety Evaluators
  - name: safety_evaluators
    prompt: |
      Create an evaluation using safety evaluators like builtin.violence,
      builtin.sexual, and builtin.hate_unfairness to check content safety.
    expected_patterns:
      - "builtin.violence"
      - "builtin.sexual"
      - "builtin.hate_unfairness"
      - "azure_ai_evaluator"
      - "data_mapping"
      - "testing_criteria"
    forbidden_patterns:
      - "ViolenceEvaluator"  # Deprecated class-based evaluator
      - "SexualEvaluator"  # Deprecated class-based evaluator
      - "from azure.ai.evaluation import"  # Deprecated SDK
    tags:
      - evaluations
      - safety
    mock_response: |
      import os
      from azure.ai.projects import AIProjectClient
      from azure.identity import DefaultAzureCredential
      from openai.types.eval_create_params import DataSourceConfigCustom
      
      with (
          DefaultAzureCredential() as credential,
          AIProjectClient(
              endpoint=os.environ["AZURE_AI_PROJECT_ENDPOINT"],
              credential=credential,
          ) as project_client,
      ):
          openai_client = project_client.get_openai_client()
          
          data_source_config = DataSourceConfigCustom(
              type="custom",
              item_schema={
                  "type": "object",
                  "properties": {
                      "query": {"type": "string"},
                      "response": {"type": "string"},
                  },
                  "required": ["query", "response"],
              },
              include_sample_schema=False,
          )
          
          # Safety evaluators
          testing_criteria = [
              {
                  "type": "azure_ai_evaluator",
                  "name": "violence_check",
                  "evaluator_name": "builtin.violence",
                  "data_mapping": {
                      "query": "{{item.query}}",
                      "response": "{{item.response}}",
                  },
              },
              {
                  "type": "azure_ai_evaluator",
                  "name": "sexual_check",
                  "evaluator_name": "builtin.sexual",
                  "data_mapping": {
                      "query": "{{item.query}}",
                      "response": "{{item.response}}",
                  },
              },
              {
                  "type": "azure_ai_evaluator",
                  "name": "hate_check",
                  "evaluator_name": "builtin.hate_unfairness",
                  "data_mapping": {
                      "query": "{{item.query}}",
                      "response": "{{item.response}}",
                  },
              },
          ]
          
          eval_object = openai_client.evals.create(
              name="Safety Evaluation",
              data_source_config=data_source_config,
              testing_criteria=testing_criteria,
          )
          print(f"Created safety evaluation: {eval_object.id}")

  # OpenAI Graders
  - name: openai_graders
    prompt: |
      Create an evaluation using OpenAI graders: label_model for classification,
      string_check for pattern matching, and text_similarity for semantic matching.
    expected_patterns:
      - '"type": "label_model"'
      - '"type": "string_check"'
      - '"type": "text_similarity"'
      - "labels"
      - "passing_labels"
      - "operation"
      - "pass_threshold"
      - "testing_criteria"
    forbidden_patterns:
      - "AzureOpenAILabelGrader"  # Deprecated class
      - "AzureOpenAIStringCheckGrader"  # Deprecated class
      - "from azure.ai.evaluation import"  # Deprecated SDK
    tags:
      - evaluations
      - graders
    mock_response: |
      import os
      from azure.ai.projects import AIProjectClient
      from azure.identity import DefaultAzureCredential
      from openai.types.eval_create_params import DataSourceConfigCustom
      
      with (
          DefaultAzureCredential() as credential,
          AIProjectClient(
              endpoint=os.environ["AZURE_AI_PROJECT_ENDPOINT"],
              credential=credential,
          ) as project_client,
      ):
          openai_client = project_client.get_openai_client()
          deployment = os.environ.get("AZURE_AI_MODEL_DEPLOYMENT_NAME", "gpt-4o-mini")
          
          data_source_config = DataSourceConfigCustom(
              type="custom",
              item_schema={
                  "type": "object",
                  "properties": {
                      "response": {"type": "string"},
                      "expected": {"type": "string"},
                  },
                  "required": ["response"],
              },
              include_sample_schema=False,
          )
          
          # OpenAI graders
          testing_criteria = [
              # Label grader for classification
              {
                  "type": "label_model",
                  "name": "sentiment_classifier",
                  "model": deployment,
                  "input": [
                      {"role": "user", "content": "Classify sentiment: {{item.response}}"}
                  ],
                  "labels": ["positive", "negative", "neutral"],
                  "passing_labels": ["positive", "neutral"],
              },
              # String check grader for pattern matching
              {
                  "type": "string_check",
                  "name": "has_disclaimer",
                  "input": "{{item.response}}",
                  "operation": "contains",
                  "reference": "Please consult a professional",
              },
              # Text similarity grader for semantic matching
              {
                  "type": "text_similarity",
                  "name": "matches_expected",
                  "input": "{{item.response}}",
                  "reference": "{{item.expected}}",
                  "evaluation_metric": "fuzzy_match",
                  "pass_threshold": 0.8,
              },
          ]
          
          eval_object = openai_client.evals.create(
              name="Graders Evaluation",
              data_source_config=data_source_config,
              testing_criteria=testing_criteria,
          )
          print(f"Created graders evaluation: {eval_object.id}")

  # List Built-in Evaluators
  - name: list_builtin_evaluators
    prompt: |
      List all available built-in evaluators in the project and get details
      about a specific evaluator including its data schema and metrics.
    expected_patterns:
      - 'project_client.evaluators.list_latest_versions'
      - 'type="builtin"'
      - "evaluators.get_version"
      - "definition.data_schema"
      - "definition.metrics"
    forbidden_patterns:
      - "from azure.ai.evaluation import"  # Deprecated SDK
    tags:
      - evaluations
      - discovery
    mock_response: |
      import os
      from azure.ai.projects import AIProjectClient
      from azure.identity import DefaultAzureCredential
      
      with (
          DefaultAzureCredential() as credential,
          AIProjectClient(
              endpoint=os.environ["AZURE_AI_PROJECT_ENDPOINT"],
              credential=credential,
          ) as project_client,
      ):
          # List all built-in evaluators
          print("Built-in Evaluators:")
          evaluators = project_client.evaluators.list_latest_versions(type="builtin")
          for e in evaluators:
              print(f"  builtin.{e.name}: {e.description}")
              print(f"    Categories: {[str(c) for c in e.categories]}")
          
          # Get specific evaluator details
          coherence = project_client.evaluators.get_version(
              name="coherence",
              version="latest"
          )
          print(f"\nCoherence Evaluator:")
          print(f"  Data Schema: {coherence.definition.data_schema}")
          print(f"  Metrics: {coherence.definition.metrics}")
