# yaml-language-server: $schema=https://json.schemastore.org/any.json
# Test scenarios for azure-ai-inference-py skill evaluation
# Each scenario tests a specific usage pattern against acceptance criteria

config:
  model: gpt-4
  max_tokens: 2000
  temperature: 0.3

scenarios:
  # Basic Chat Completions with Entra ID
  - name: basic_chat_completion_default_credential
    prompt: |
      Create a basic Azure AI Inference chat completions example using
      DefaultAzureCredential with SystemMessage and UserMessage. Include
      printing the response content.
    expected_patterns:
      - "DefaultAzureCredential"
      - "ChatCompletionsClient"
      - "SystemMessage"
      - "UserMessage"
      - "complete"
    forbidden_patterns:
      - 'openai\.OpenAI'
      - 'client\.chat\.completions'
    tags:
      - basic
      - authentication
      - chat
    mock_response: |
      import os
      from azure.identity import DefaultAzureCredential
      from azure.ai.inference import ChatCompletionsClient
      from azure.ai.inference.models import SystemMessage, UserMessage
      
      client = ChatCompletionsClient(
          endpoint=os.environ["AZURE_INFERENCE_ENDPOINT"],
          credential=DefaultAzureCredential(),
      )
      
      response = client.complete(
          messages=[
              SystemMessage(content="You are a helpful assistant."),
              UserMessage(content="What is Azure AI?"),
          ],
          model=os.environ.get("AZURE_INFERENCE_MODEL"),
      )
      
      print(response.choices[0].message.content)

  # API Key Authentication
  - name: api_key_chat_completion
    prompt: |
      Create a chat completions example using AzureKeyCredential with
      environment variables for the endpoint and credential.
    expected_patterns:
      - "AzureKeyCredential"
      - "AZURE_INFERENCE_CREDENTIAL"
      - "ChatCompletionsClient"
    forbidden_patterns:
      - 'AzureKeyCredential\(\"'
    tags:
      - authentication
      - api-key
    mock_response: |
      import os
      from azure.ai.inference import ChatCompletionsClient
      from azure.ai.inference.models import SystemMessage, UserMessage
      from azure.core.credentials import AzureKeyCredential
      
      client = ChatCompletionsClient(
          endpoint=os.environ["AZURE_INFERENCE_ENDPOINT"],
          credential=AzureKeyCredential(os.environ["AZURE_INFERENCE_CREDENTIAL"]),
      )
      
      response = client.complete(
          messages=[
              SystemMessage(content="You are a helpful assistant."),
              UserMessage(content="Give me one fact about Azure."),
          ],
      )
      
      print(response.choices[0].message.content)

  # Embeddings
  - name: embeddings_basic
    prompt: |
      Create an embeddings example using EmbeddingsClient and embed
      a list of text inputs, then print the embedding length.
    expected_patterns:
      - "EmbeddingsClient"
      - "embed"
      - "response.data"
    forbidden_patterns:
      - 'client\.embeddings'
    tags:
      - embeddings
      - basic
    mock_response: |
      import os
      from azure.ai.inference import EmbeddingsClient
      from azure.core.credentials import AzureKeyCredential
      
      client = EmbeddingsClient(
          endpoint=os.environ["AZURE_INFERENCE_ENDPOINT"],
          credential=AzureKeyCredential(os.environ["AZURE_INFERENCE_CREDENTIAL"]),
      )
      
      response = client.embed(
          input=["first phrase", "second phrase"],
          model="text-embedding-3-small",
      )
      
      embedding = response.data[0].embedding
      print(len(embedding))

  # Streaming Chat Completions
  - name: streaming_chat_completions
    prompt: |
      Create a streaming chat completions example using stream=True and
      print the delta content as it arrives.
    expected_patterns:
      - "stream=True"
      - "for update in response"
      - "delta.content"
    forbidden_patterns:
      - "stream=False"
    tags:
      - streaming
      - chat
    mock_response: |
      import os
      from azure.ai.inference import ChatCompletionsClient
      from azure.ai.inference.models import SystemMessage, UserMessage
      from azure.core.credentials import AzureKeyCredential
      
      client = ChatCompletionsClient(
          endpoint=os.environ["AZURE_INFERENCE_ENDPOINT"],
          credential=AzureKeyCredential(os.environ["AZURE_INFERENCE_CREDENTIAL"]),
      )
      
      response = client.complete(
          stream=True,
          messages=[
              SystemMessage(content="You are a helpful assistant."),
              UserMessage(content="Write a short poem about Azure."),
          ],
      )
      
      for update in response:
          if update.choices and update.choices[0].delta:
              print(update.choices[0].delta.content or "", end="", flush=True)

  # Tool Calling Flow
  - name: tool_calling_flow
    prompt: |
      Create a tool calling flow using ChatCompletionsToolDefinition and
      FunctionDefinition. Show how to append AssistantMessage and ToolMessage
      when tools are called.
    expected_patterns:
      - "ChatCompletionsToolDefinition"
      - "FunctionDefinition"
      - "ToolMessage"
      - "tool_calls"
    forbidden_patterns:
      - "function_call"
    tags:
      - tools
      - chat
    mock_response: |
      import json
      import os
      from azure.ai.inference import ChatCompletionsClient
      from azure.ai.inference.models import (
          SystemMessage,
          UserMessage,
          AssistantMessage,
          ToolMessage,
          ChatCompletionsToolDefinition,
          FunctionDefinition,
      )
      from azure.core.credentials import AzureKeyCredential
      
      client = ChatCompletionsClient(
          endpoint=os.environ["AZURE_INFERENCE_ENDPOINT"],
          credential=AzureKeyCredential(os.environ["AZURE_INFERENCE_CREDENTIAL"]),
      )
      
      tools = [
          ChatCompletionsToolDefinition(
              function=FunctionDefinition(
                  name="get_weather",
                  description="Get current weather for a location",
                  parameters={
                      "type": "object",
                      "properties": {"location": {"type": "string"}},
                      "required": ["location"],
                  },
              )
          )
      ]
      
      messages = [
          SystemMessage(content="You are a helpful assistant."),
          UserMessage(content="What's the weather in Seattle?"),
      ]
      
      response = client.complete(messages=messages, tools=tools)
      assistant_message = response.choices[0].message
      
      if assistant_message.tool_calls:
          messages.append(AssistantMessage(
              content=assistant_message.content,
              tool_calls=assistant_message.tool_calls,
          ))
          for tool_call in assistant_message.tool_calls:
              result = {"location": "Seattle", "temp": 72}
              messages.append(ToolMessage(
                  tool_call_id=tool_call.id,
                  content=json.dumps(result),
              ))
      
      final_response = client.complete(messages=messages, tools=tools)
      print(final_response.choices[0].message.content)

  # Async Chat Completions
  - name: async_chat_completions
    prompt: |
      Create an async chat completions example using azure.ai.inference.aio
      with DefaultAzureCredential from azure.identity.aio and proper awaits.
    expected_patterns:
      - 'from azure\.ai\.inference\.aio import ChatCompletionsClient'
      - 'from azure\.identity\.aio import DefaultAzureCredential'
      - 'await client\.complete'
    forbidden_patterns:
      - 'from azure\.ai\.inference import ChatCompletionsClient'
      - 'from azure\.identity import DefaultAzureCredential'
    tags:
      - async
      - chat
    mock_response: |
      import os
      import asyncio
      from azure.ai.inference.aio import ChatCompletionsClient
      from azure.ai.inference.models import SystemMessage, UserMessage
      from azure.identity.aio import DefaultAzureCredential
      
      async def main():
          client = ChatCompletionsClient(
              endpoint=os.environ["AZURE_INFERENCE_ENDPOINT"],
              credential=DefaultAzureCredential(),
          )
          try:
              response = await client.complete(
                  messages=[
                      SystemMessage(content="You are a helpful assistant."),
                      UserMessage(content="Tell me a fun Azure fact."),
                  ],
              )
              print(response.choices[0].message.content)
          finally:
              await client.close()
      
      asyncio.run(main())

  # Async Streaming
  - name: async_streaming_chat_completions
    prompt: |
      Create an async streaming chat completions example using stream=True
      and async for to iterate over updates.
    expected_patterns:
      - "stream=True"
      - "async for update in response"
      - "delta.content"
    forbidden_patterns:
      - "\n        for update in response"  # Sync for loop (not async for)
    tags:
      - async
      - streaming
    mock_response: |
      import os
      import asyncio
      from azure.ai.inference.aio import ChatCompletionsClient
      from azure.ai.inference.models import UserMessage
      from azure.core.credentials import AzureKeyCredential
      
      async def main():
          client = ChatCompletionsClient(
              endpoint=os.environ["AZURE_INFERENCE_ENDPOINT"],
              credential=AzureKeyCredential(os.environ["AZURE_INFERENCE_CREDENTIAL"]),
          )
          try:
              response = await client.complete(
                  stream=True,
                  messages=[UserMessage(content="Stream this response.")],
              )
              async for update in response:
                  if update.choices:
                      content = update.choices[0].delta.content
                      if content:
                          print(content, end="", flush=True)
          finally:
              await client.close()
      
      asyncio.run(main())

  # load_client usage
  - name: load_client_usage
    prompt: |
      Show how to use load_client to create a client and print its type.
      Use AzureKeyCredential and environment variables.
    expected_patterns:
      - "load_client"
      - "type(client).__name__"
    forbidden_patterns:
      - 'ChatCompletionsClient\('
    tags:
      - load-client
      - basic
    mock_response: |
      import os
      from azure.ai.inference import load_client
      from azure.core.credentials import AzureKeyCredential
      
      client = load_client(
          endpoint=os.environ["AZURE_INFERENCE_ENDPOINT"],
          credential=AzureKeyCredential(os.environ["AZURE_INFERENCE_CREDENTIAL"]),
      )
      
      print(f"Created client of type: {type(client).__name__}")
