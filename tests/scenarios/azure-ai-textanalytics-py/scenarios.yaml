# Test scenarios for azure-ai-textanalytics-py skill evaluation
# Each scenario tests a specific usage pattern against acceptance criteria

config:
  model: gpt-4
  max_tokens: 2000
  temperature: 0.3

scenarios:
  # Basic Sentiment Analysis
  - name: basic_sentiment_analysis
    prompt: |
      Create a basic Azure Text Analytics example that analyzes sentiment
      of multiple documents. Include proper authentication with DefaultAzureCredential,
      error handling, and cleanup.
    expected_patterns:
      - "DefaultAzureCredential"
      - "TextAnalyticsClient"
      - "analyze_sentiment"
      - "is_error"
      - "confidence_scores"
    forbidden_patterns:
      - "hardcoded"
      - "api_key ="
    tags:
      - basic
      - sentiment
      - authentication
    mock_response: |
      import os
      from azure.ai.textanalytics import TextAnalyticsClient
      from azure.identity import DefaultAzureCredential
      
      endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
      client = TextAnalyticsClient(endpoint, DefaultAzureCredential())
      
      documents = [
          "I had a wonderful experience at this restaurant!",
          "The service was terrible and the food was cold."
      ]
      
      result = client.analyze_sentiment(documents)
      
      for doc in result:
          if not doc.is_error:
              print(f"Sentiment: {doc.sentiment}")
              print(f"Positive: {doc.confidence_scores.positive:.2f}")
              print(f"Negative: {doc.confidence_scores.negative:.2f}")
              print(f"Neutral: {doc.confidence_scores.neutral:.2f}")

  # Sentiment Analysis with Opinion Mining
  - name: sentiment_with_opinion_mining
    prompt: |
      Create an example that analyzes sentiment with opinion mining (aspect-based sentiment).
      Extract both the target and assessments for each opinion.
    expected_patterns:
      - "show_opinion_mining=True"
      - "mined_opinions"
      - "opinion.target"
      - "opinion.assessments"
    forbidden_patterns:
      - "show_opinion_mining = True"
    tags:
      - sentiment
      - advanced
    mock_response: |
      import os
      from azure.ai.textanalytics import TextAnalyticsClient
      from azure.identity import DefaultAzureCredential
      
      client = TextAnalyticsClient(
          os.environ["AZURE_LANGUAGE_ENDPOINT"],
          DefaultAzureCredential()
      )
      
      documents = ["The hotel was beautiful but the service was poor."]
      
      result = client.analyze_sentiment(documents, show_opinion_mining=True)
      
      for doc in result:
          if not doc.is_error:
              for sentence in doc.sentences:
                  for opinion in sentence.mined_opinions:
                      target = opinion.target
                      print(f"Target: {target.text} ({target.sentiment})")
                      for assessment in opinion.assessments:
                          print(f"  Assessment: {assessment.text} ({assessment.sentiment})")

  # Named Entity Recognition
  - name: named_entity_recognition
    prompt: |
      Create an example that recognizes named entities in text including
      organizations, people, and locations. Include confidence scores
      and proper error handling.
    expected_patterns:
      - "recognize_entities"
      - "entity.text"
      - "entity.category"
      - "entity.confidence_score"
    forbidden_patterns:
      - "recognize_entity"  # Wrong method name
      - "entity.type"  # Wrong attribute
    tags:
      - entities
      - ner
    mock_response: |
      import os
      from azure.ai.textanalytics import TextAnalyticsClient
      from azure.identity import DefaultAzureCredential
      
      client = TextAnalyticsClient(
          os.environ["AZURE_LANGUAGE_ENDPOINT"],
          DefaultAzureCredential()
      )
      
      documents = [
          "Microsoft was founded by Bill Gates and Paul Allen in Seattle."
      ]
      
      result = client.recognize_entities(documents)
      
      for doc in result:
          if not doc.is_error:
              for entity in doc.entities:
                  print(f"Entity: {entity.text}")
                  print(f"  Category: {entity.category}")
                  print(f"  Subcategory: {entity.subcategory}")
                  print(f"  Confidence: {entity.confidence_score:.2f}")

  # PII Detection and Redaction
  - name: pii_detection
    prompt: |
      Create an example that detects personally identifiable information (PII)
      and redacts it from text. Include proper error handling.
    expected_patterns:
      - "recognize_pii_entities"
      - "redacted_text"
      - "PiiEntityDomain"
      - "is_error"
    tags:
      - pii
      - security
    mock_response: |
      import os
      from azure.ai.textanalytics import TextAnalyticsClient
      from azure.identity import DefaultAzureCredential
      
      client = TextAnalyticsClient(
          os.environ["AZURE_LANGUAGE_ENDPOINT"],
          DefaultAzureCredential()
      )
      
      documents = [
          "My email is john@example.com and my phone is 555-123-4567.",
          "Social Security Number: 123-45-6789"
      ]
      
      result = client.recognize_pii_entities(documents)
      
      for doc in result:
          if not doc.is_error:
              print(f"Redacted: {doc.redacted_text}")
              for entity in doc.entities:
                  print(f"PII: {entity.text}")
                  print(f"  Category: {entity.category}")

  # Key Phrase Extraction
  - name: key_phrase_extraction
    prompt: |
      Create an example that extracts key phrases from documents.
      Include multiple documents with proper error handling.
    expected_patterns:
      - "extract_key_phrases"
      - "doc.key_phrases"
      - "is_error"
    forbidden_patterns:
      - "extract_keywords"  # Wrong method name
    tags:
      - key-phrases
    mock_response: |
      import os
      from azure.ai.textanalytics import TextAnalyticsClient
      from azure.identity import DefaultAzureCredential
      
      client = TextAnalyticsClient(
          os.environ["AZURE_LANGUAGE_ENDPOINT"],
          DefaultAzureCredential()
      )
      
      documents = [
          "Azure AI provides powerful machine learning and NLP capabilities.",
          "The natural language processing models deliver state-of-the-art results."
      ]
      
      result = client.extract_key_phrases(documents)
      
      for doc in result:
          if not doc.is_error:
              print(f"Key phrases: {doc.key_phrases}")

  # Language Detection
  - name: language_detection
    prompt: |
      Create an example that detects the language of multiple documents
      in different languages. Include accessing language name and ISO 639-1 code.
    expected_patterns:
      - "detect_language"
      - "primary_language"
      - "iso6391_name"
      - "confidence_score"
    forbidden_patterns:
      - "iso639_name"  # Wrong: missing "1" in 6391
      - "language_name"  # Wrong attribute
    tags:
      - language-detection
    mock_response: |
      import os
      from azure.ai.textanalytics import TextAnalyticsClient
      from azure.identity import DefaultAzureCredential
      
      client = TextAnalyticsClient(
          os.environ["AZURE_LANGUAGE_ENDPOINT"],
          DefaultAzureCredential()
      )
      
      documents = [
          "Hello, how are you today?",
          "Bonjour, comment allez-vous?",
          "Hola, ¿cómo estás?"
      ]
      
      result = client.detect_language(documents)
      
      for doc in result:
          if not doc.is_error:
              lang = doc.primary_language
              print(f"Language: {lang.name}")
              print(f"ISO 639-1: {lang.iso6391_name}")
              print(f"Confidence: {lang.confidence_score:.2f}")

  # Batch Analysis with Multiple Actions
  - name: batch_analysis_multiple_actions
    prompt: |
      Create an example that performs multiple analyses (sentiment, entities, key phrases)
      in a single batch operation using begin_analyze_actions. Include proper
      polling and result iteration.
    expected_patterns:
      - "begin_analyze_actions"
      - "AnalyzeSentimentAction"
      - "RecognizeEntitiesAction"
      - "ExtractKeyPhrasesAction"
      - "poller.result()"
      - "result.kind =="
    forbidden_patterns:
      - "(?<!begin_)analyze_actions"  # Missing "begin_"
    tags:
      - batch
      - advanced
    mock_response: |
      import os
      from azure.ai.textanalytics import (
          TextAnalyticsClient,
          AnalyzeSentimentAction,
          RecognizeEntitiesAction,
          ExtractKeyPhrasesAction,
      )
      from azure.identity import DefaultAzureCredential
      
      client = TextAnalyticsClient(
          os.environ["AZURE_LANGUAGE_ENDPOINT"],
          DefaultAzureCredential()
      )
      
      documents = ["Microsoft announced new AI capabilities."]
      
      poller = client.begin_analyze_actions(
          documents,
          actions=[
              AnalyzeSentimentAction(),
              RecognizeEntitiesAction(),
              ExtractKeyPhrasesAction(),
          ],
      )
      
      results = poller.result()
      
      for doc_results in results:
          for result in doc_results:
              if result.kind == "SentimentAnalysis":
                  print(f"Sentiment: {result.sentiment}")
              elif result.kind == "EntityRecognition":
                  print(f"Entities: {[e.text for e in result.entities]}")
              elif result.kind == "KeyPhraseExtraction":
                  print(f"Key phrases: {result.key_phrases}")

  # Async Client Usage
  - name: async_client_usage
    prompt: |
      Create an async version using the async TextAnalyticsClient from azure.ai.textanalytics.aio.
      Include proper async context management and await patterns.
    expected_patterns:
      - "from azure.ai.textanalytics.aio import TextAnalyticsClient"
      - "async with"
      - "await client.analyze_sentiment"
      - "asyncio.run"
    forbidden_patterns:
      - "from azure.ai.textanalytics import TextAnalyticsClient"  # Should be .aio
      - "with client:"  # Should be async with
    tags:
      - async
      - advanced
    mock_response: |
      import os
      import asyncio
      from azure.ai.textanalytics.aio import TextAnalyticsClient
      from azure.identity.aio import DefaultAzureCredential
      
      async def main():
          async with TextAnalyticsClient(
              os.environ["AZURE_LANGUAGE_ENDPOINT"],
              DefaultAzureCredential()
          ) as client:
              documents = ["This is a wonderful product!"]
              
              result = await client.analyze_sentiment(documents)
              
              for doc in result:
                  if not doc.is_error:
                      print(f"Sentiment: {doc.sentiment}")
      
      asyncio.run(main())

  # Healthcare Entity Recognition
  - name: healthcare_entity_recognition
    prompt: |
      Create an example that analyzes healthcare entities using begin_analyze_healthcare_entities.
      Include proper polling for the long-running operation and accessing entity relationships.
    expected_patterns:
      - "begin_analyze_healthcare_entities"
      - "poller.result()"
      - "entity.normalized_text"
      - "entity.data_sources"
    tags:
      - healthcare
      - advanced
    mock_response: |
      import os
      from azure.ai.textanalytics import TextAnalyticsClient
      from azure.identity import DefaultAzureCredential
      
      client = TextAnalyticsClient(
          os.environ["AZURE_LANGUAGE_ENDPOINT"],
          DefaultAzureCredential()
      )
      
      documents = [
          "Patient has hypertension and was prescribed lisinopril 10mg daily."
      ]
      
      poller = client.begin_analyze_healthcare_entities(documents)
      result = poller.result()
      
      for doc in result:
          if not doc.is_error:
              for entity in doc.entities:
                  print(f"Entity: {entity.text}")
                  print(f"  Category: {entity.category}")
                  print(f"  Normalized: {entity.normalized_text}")
                  
                  if entity.data_sources:
                      for source in entity.data_sources:
                          print(f"  Source: {source.name} - ID: {source.entity_id}")
