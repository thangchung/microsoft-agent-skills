name: Smoke Test

# Fast, deterministic testing for PRs and pushes
# Full evaluation with real SDK runs nightly via skill-evaluation.yml

on:
  pull_request:
  push:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: pnpm/action-setup@v4
        with:
          version: 9

      - uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'pnpm'
          cache-dependency-path: tests/pnpm-lock.yaml

      - name: Install dependencies
        working-directory: tests
        run: pnpm install

      - name: TypeScript check
        working-directory: tests
        run: pnpm typecheck

      - name: Run unit tests
        working-directory: tests
        run: pnpm test:run

      - name: Run skill evaluations (all skills)
        id: harness
        working-directory: tests
        run: |
          # Run harness and capture exit code (don't fail the step yet)
          set +e
          pnpm harness --all --mock --output markdown --output-file results.md
          HARNESS_EXIT=$?
          pnpm harness --all --mock --output json --output-file results.json
          set -e
          
          # Store result for later steps
          echo "exit_code=$HARNESS_EXIT" >> $GITHUB_OUTPUT
          
          # Always exit 0 here so we can write summary
          exit 0

      - name: Write job summary
        if: always()
        working-directory: tests
        run: |
          if [ -f results.md ]; then
            cat results.md >> $GITHUB_STEP_SUMMARY
          else
            echo "⚠️ No results file found" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "ℹ️ This is a **smoke test** using mock responses for fast CI feedback." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Full evaluation with real Copilot SDK runs **nightly at 2 AM UTC** via [Skill Evaluation](../actions/workflows/skill-evaluation.yml)." >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ steps.harness.outputs.exit_code }}" != "0" ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "⚠️ **Some skills have failing scenarios.** See details above." >> $GITHUB_STEP_SUMMARY
          fi

      - name: Check evaluation results
        if: always()
        run: |
          if [ "${{ steps.harness.outputs.exit_code }}" != "0" ]; then
            echo "::warning::Some skills have failing scenarios"
            # Don't fail the job - this is informational during development
            # Uncomment the line below to enforce passing evaluations:
            # exit 1
          fi

      - name: Upload results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: evaluation-results
          path: |
            tests/results.md
            tests/results.json
